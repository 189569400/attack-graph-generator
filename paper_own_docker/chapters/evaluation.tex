\section{EVALUATION}

\begin{table*}
	\begin{center}
		\begin{tabular}{ |p{20mm}|p{25mm}|p{20mm}|p{10mm}|c|p{60mm}| } 
			\hline
			Name & Description & Technology stack & No. Containers & No. vuln. & Github link \\\hline 
			
			Netflix OSS & Combination of containers provided from Netflix. & Spring Cloud, Netflix Ribbon, Spring Cloud Netflix, Netflix's Eureka & 10 & 4111 & https://github.com/Oreste-Luci/netflix-oss-example \\\hline
			
			Atsea Sample Shop App & An example online store application. & Spring Boot, React, NGINX, PostgreSQL & 4 & 120 &  https://github.com/dockersamples/atsea-sample-shop-app \\\hline
			
			JavaEE demo & An application for browsing movies along with other related functions. & Java EE application, React, Tomcat EE & 2 & 149 &  https://github.com/dockersamples/javaee-demo \\\hline
			
			PHPMailer and Samba & An artificial example created from two separate containers. We use an augmented version for the scalability tests. & PHPMailer(email creation and transfer class for PHP), Samba(SMB/CIFS networking protocol) & 2 & 548 &  https://github.com/opsxcq/exploit-CVE-2016-10033
			https://github.com/opsxcq/exploit-CVE-2017-7494 \\\hline
			
			
			\hline
		\end{tabular}
	\end{center}
	
	\caption{Microservice architecture examples analyzed by the attack graph generator.}
	\label{table_technologies}
	
\end{table*}

Real-world microservice architectures are composed of many containers that run different technologies with various degrees of connectivity between each other. This raises the need for a robust and scalable attack graph system. In Subsection \ref{chap:heterogenious_systems}, we first show different microservice architectures on which our system was tested on. We then have a look at how others evaluate their systems. Finally in Subsection \ref{chap:scalability_eval} we conduct few experiments in order to test the scalability of our system with different number of containers and connectivity. All of the experiments were performed on a Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz with 8GB of RAM running Ubuntu 16.04.3 LTS.

\subsection{Heterogenious microservice systems evaluation}
\label{chap:heterogenious_systems}
Modern microservice architectures use an abundance of different technologies, number of containers, various connectivity and number of vulnerabilities. Therefore it is of immense importance to show that the attack graph generator works in such heterogenious scenarios. In order to do this we ran our system on some real and slightly modified Github examples as described on Table \ref{table_technologies} composed of different types of technologies, number of containers and vulnerabilities. The examples are as follows: NetflixOSS, Atsea Sample Shop App, and JavaEE demo. NetflixOSS is a microservice system provided by Netflix that is composed of 10 containers and uses Spring Cloud, Netflix Ribbon, Netflix Eureka etc. Atsea Sample Shop App is an e-commerce sample web application composed of 4 containers and that uses Spring Boot, React, NGINX and PostgreSQL. JavaEE demo is a sample application for browsing movies that is composed of only 2 containers and uses JavaEE, React and Tomcat EE. We ran the attack graph generator and checked for correctness of the resulting attack graph based on domain knowledge. After running the attack graph generator, the structure of the resulting attack graphs (especially in the Netflix case) was quite linear. This linearity is because each container is connected to few other containers to reduce unnecessary communication and increase encapsulation. So based on this connectivity an attacker needs to perform multiple intermediate steps in order to reach the target container. Additionally we noted that the running time of our system for each of these examples was short, and additional tests to test the scalability were needed. Phpmailer and Samba is an artificial example that we use and extend in the following subsection to perform these scalability tests.
	
\begin{table*}
	\begin{center}
		\begin{tabular}{ |c|c|c|c|c|c| } 
			\hline
			Statistics & example\_20 & example\_50 & example\_100 & example\_500 & example\_1000 \\ 
			
			No. of Phpmailer containers & 1 & 1 & 1 & 1 & 1 \\ 
			
			No. of Samba containers & 20 & 50 & 100 & 500 & 1000 \\ 
			
			No. of nodes in topology & 23 & 53 & 103 & 503 & 1003\\ 
			
			No. of edges in topology & 253 & 1378 & 5253 & 126253 & 502503 \\ 
			
			No. nodes in attack graph & 43 & 103 & 203 & 1003 & 2003 \\ 
			
			No. edges in attack graph & 863 & 5153 & 20303 & 501503 & 2003003 \\ 
			
			Topology parsing time & 0.02879 & 0.0563 & 0.1241 & 0.7184 & 2.3664 \\ 
			
			Vulnerability preprocessing time & 0.5377 & 0.9128 & 1.6648 & 6.9961 & 15.0639 \\ 
			
			Breadth-First Search time & 0.2763 & 1.6524 & 6.5527 & 165.3634 & 767.5539 \\ 
			
			Total time & 0.8429 & 2.6216 & 8.3417 & 173.0781 & 784.9843 \\ 
			\hline
		\end{tabular}
	\end{center}
	
	\caption{Scalability experiments with the graph characteristics and execution times. The times are given in seconds.}
	
	\label{table_scalability}
\end{table*}

\subsection{Scalability evaluation}
\label{chap:scalability_eval}
Extensive scalability study of attack graph generators  is rare in current literature and many parameters contribute to the complexity of a comprehensive analysis. Parameters that usually vary in this sort of evaluation are the number of nodes, their connectivity and the amount of vulnerabilities per container. All of these components contribute to the execution time of a given algorithm. Even though the definitions of an attack graph vary, we hope to reach a comprehensive comparison with current methods. In this case we compare our system to existing work by treating every container as a host machine, and any physical connection between two machines as a connection between two containers. In the following text, we first look at three works and their scalability evaluation results. After this comparison, we present the scalability results of our system.

Seyner in his work tests the system in both a small and extended examples \cite{sheyner2002automated}. The attack graph in the larger example has 5948 nodes and 68364 edges. The time needed for NuSMV to execute this configuration is 2 hours, but the model checking part took 4 minutes. Sheyner claims that the performance bottleneck is inside the graph generation procedure. 

Ingols tested his system on a network of 250 hosts. He afterwards continued his study on a simulated network of 50000 hosts in under 4 minutes \cite{ingols2006practical}. Although this method yields better performance than the aforementioned approach, this evaluation is based on the Multiple Prerequisite graph, which is different from ours. In addition to this, missing explanation of how the hosts are connected, does not make it directly comparable to our method.

Ou provides some more extended study where he tests his system on more examples \cite{ou2006scalable}. He mentions that the asymptotic CPU time is between $O(n^2)$ and $O(n^3)$, where n is the number of hosts. The performance of the system for 1000 fully connected nodes takes more than 1000 seconds to execute. He also provides an evaluation where he MulVAL clearly outperforms the Sheyner's system.





In our scalability experiments we use Samba \cite{samba} and Phpmailer \cite{phpmailer} containers which were taken from their respective Github repositories. We extended this example and artificially made fully connected topologies of 20, 50, 100, 500 and 1000 Samba containers to test the scalability of the system. The Phpmailer container has 181 vulnerabilities, while the Samba container has 367 vulnerabilities detected by Clair. In our tests, we report the total execution time as well as its components times: Topology parsing time, Vulnerability preprocessing time and Breath-First Search time. The total time contains the topology parsing, the attack graph generation and some minor utility processes. The Topology parsing time is the time required to generate the graph topology. The Vulnerability preprocessing time is the time required to convert the vulnerabilities into sets of pre- and postconditions. The Breath-First Search time is the time needed for Breadth-First Search to traverse the topology and  generate the attack graph after the previous steps are done. All of the components are executed five times for each of the examples and their final time is averaged. The times are given in seconds.  However, the total time does not include the vulnerability analysis by Clair. Evaluation of Clair can depend on multiple factors and it is therefore not in the scope of this analysis.

Table \ref{table_scalability} shows the results of our experiments. In each of these experiments the number of Phpmailer containers stays constant, while the number of Samba containers is increasing. This increase is done in a fully connected fashion, where a node of each container is connected to every other container. In addition, there are also two additional artificial containers("outside" that represents the environment from where the attacker can attack and the "docker host", i.e. the docker daemon where the containers are hosted). Therefore the number of nodes in the topology graph is the sum of: "outside", "docker host", number of Phpmailer containers and number of Samba containers. The number of edges of the topology graph is a a combination of: 1 edge("outside"-"Phpmailer"), n edges("docker host" to all of the containers) and clique of the Phpmailer and samba containers n*(n+1)/2. For example\_5, the number of containers would be 8(1 Phpmailer, 1 outside, 1 docker host and 5 Samba containers) the number of edges in the topology graph would be 32: 1 outside edge, 6 docker host edges(n=6, 1 Phpmailer and 5 Sambas) and 25 clique edges(5*6/2=15).

Throughout the experiments, for the smaller configurations, the biggest time bottleneck is the preprocessing step. However this step increases in linear fashion because the container files are analyzed only once by Clair. The attack graph generation for the smaller examples is considerably less than the preprocessing time. Starting from example\_500, we can notice sharp increase in BDF execution time to 165 seconds. For the previous example with example\_100, needed attack graph generation time is 6.5 seconds.


